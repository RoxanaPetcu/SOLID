# --------------------------------------------------------------------------------
# Project Name: SOLID
# File Name: solid_intent_aware_conversation_generation.py
# --------------------------------------------------------------------------------
# Authors of this file: Roxana Petcu, Arian Askari
# Contributions:
#   Roxana Petcu - Initial framework and logic for generating data
#   Arian Askari - Optimized the algorithm and added multi-intent self-instructig
# --------------------------------------------------------------------------------
# This project is the result of collaborative efforts from the authors mentioned.
# --------------------------------------------------------------------------------
import os
import json
import tqdm
import sys

"""# Model"""
from transformers import AutoTokenizer, LlamaForCausalLM, AutoModelForCausalLM
model_name = "HuggingFaceH4/zephyr-7b-beta"
tokenizer = AutoTokenizer.from_pretrained(model_name, truncation=True, padding=True, padding_side="left", maximum_length = 2048, model_max_length = 2048)
model = AutoModelForCausalLM.from_pretrained(model_name, load_in_4bit = True, device_map = 'auto')
tokenizer.pad_token = tokenizer.eos_token
model.generation_config.pad_token_id = model.generation_config.eos_token_id

"""# Utility Functions

## Generate and filter next turn
"""

def turn_generation(message):
    """Receives a message and prompts the model, it returns the result"""

    tokens = tokenizer(message, return_tensors="pt", truncation=True, padding=True).to(0)
    outputs = model.generate(input_ids=tokens["input_ids"], attention_mask= tokens["attention_mask"], max_new_tokens = 100, eos_token_id= tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id)
    str_output = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return str_output

def filter_new_turn(text, original_message, keyword1, keyword2):
    """Filter new generation by: 1) removing artifacts generated by the LLM (tendency to generate multiple turns)"""

    # Find the start of the new turn
    start_new_turn = text.find(original_message) + len(original_message)
    new_turn = text[start_new_turn:]

    # Check for the presence of both keywords in the new turn
    keyword1_in_new_turn = keyword1 in new_turn
    keyword2_in_new_turn = keyword2 in new_turn

    if keyword1_in_new_turn and keyword2_in_new_turn:                                           # If both keywords are found, keep the part before the first occurrence of either keyword
        first_keyword_index = min(new_turn.find(keyword1), new_turn.find(keyword2))
        new_turn = new_turn[:first_keyword_index]
    elif keyword1_in_new_turn:                                                                  # If only keyword1 is found, keep the part before its occurrence
        new_turn = new_turn[:new_turn.find(keyword1)]
    elif keyword2_in_new_turn:                                                                  # If only keyword2 is found, keep the part before its occurrence
        new_turn = new_turn[:new_turn.find(keyword2)]

    return text[:start_new_turn] + new_turn

def trim_to_last_punctuation(text):
    """
    Trims a string to include only the content up to the last punctuation mark ('.', '?', '!').
    After that, it removes empty lines.
    """

    # Finding the last occurrence of punctuation marks
    periods = text.rfind('.')
    questions = text.rfind('?')
    exclamations = text.rfind('!')

    # Identifying the position of the last punctuation mark
    last_punctuation = max(periods, questions, exclamations)

    # Trim the text up to the last punctuation mark
    if last_punctuation != -1:
        text = text[:last_punctuation + 1]

    # Remove all empty lines
    text = '\n'.join([line for line in text.split('\n') if line.strip() != ''])

    return text

"""## Generate indices for turn in dialogue"""

def split_list_items_by_underscore(strings):
    """
    Split each string in a list by the underscore symbol '_',
    keeping the underscore attached to the right-hand side of each split.

    Parameters:
    strings (list): The list of strings to be split.

    Returns:
    list: A new list of strings obtained by splitting the original strings by '_'.
    """
    split_result = []
    for string in strings:
        # Check if the string contains an underscore
        if '_' in string:
            parts = string.split('_')
            split_result.append(parts[0])  # Append the first part as is
            for part in parts[1:]:
                split_result.append('_' + part)  # Append the remaining parts with an underscore
        else:
            split_result.append(string)

    return split_result

def get_turn(strings):
    """
    Receives a list of intents and returns the turn in the conversation (either 0 or 1)
    """

    counter = -1

    def update_counter(s):
        nonlocal counter
        counter += not s.startswith("_")
        return counter%2

    return list(map(update_counter, strings))

"""## Utility functions for complex intents (for ex: PA_IR, PA_IR_OQ)"""

import re

def extract_last_two_turns(dialogue):
    """
    Extract the last two turns from a dialogue where each turn starts with a keyword followed by the symbol ":".

    Parameters:
    dialogue (str): The dialogue string.

    Returns:
    tuple: A tuple containing the conversation history without the last two turns, and a list of the last two turns.
    """

    # Split the dialogue into turns based on the symbol ":"
    turns = dialogue.split(":")

    # Check if there are at least two turns, else return the original dialogue and an empty list
    if len(turns) < 3:
        return dialogue, []

    # The last element of the split will always be empty, remove it
    if not turns[-1].strip():
        turns.pop()

    # Extract the last two turns
    last_two_turns = [turns[-2].strip(), turns[-1].strip()]

    # Remove the last two turns from the dialogue
    remaining_dialogue = ":".join(turns[:-2])
    remaining_dialogue = "\n".join(remaining_dialogue.split("\n")[:-1])

    last_two_turns[0] = last_two_turns[0].split(":", 1)[-1].strip()
    last_two_turns[1] = last_two_turns[1].split(":", 1)[-1].strip()

    return remaining_dialogue, [last_two_turns[0].split('\n', 1)[0], last_two_turns[1].split('\n', 1)[0]]

def combine(last_two_turns, previous_reply, first_sentence = "Response", second_sentence = "Question"):

  str_output = previous_reply.replace(".", ":") + " " + last_two_turns[0] + " " + last_two_turns[1]
  return '\n' + str_output

def combine_instruction(intent, instructions_dict, intents_dict_secondkey):
  content = ""
  for i in intent.split("_"):
    instruction = instructions_dict[i][intents_dict_secondkey]
    content = content + " " + instruction
  content = content.strip()
  combine_prompt = """Provide an instruction based on below content:
  Content: {}
  Instruction: """.format(content)
  tokens = tokenizer(combine_prompt, return_tensors="pt", truncation=True, padding=True).to(0)
  outputs = model.generate(input_ids=tokens["input_ids"], attention_mask= tokens["attention_mask"], max_new_tokens = 100, eos_token_id= tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id)
  str_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
  combined_instruction = str_output.split("Instruction: ")[1].replace("\n", " ").replace("\r", " ").strip()
  return combined_instruction
def combine_instructionv2(intent, instructions_dict, intents_dict_secondkey):
  content = ""
  middle_inputs = []
  for rank, i in enumerate(intent.split("_")):
    instruction = instructions_dict[i][intents_dict_secondkey]
    content = content + " " + instruction
    middle_inputs.append("Instruction {}: {}\n".format(rank, instruction))
  # print("middle_inputs: ", middle_inputs)
  content = content.strip()
  combine_prompt = """Example1:
Instruction 1: Reply with more details in conversation style.
Instruction 2: Convey dissatisfaction for the previous response.
Merged Instruction: In a conversational style, reply with more details and express dissatisfaction for the previous response.

Example2:
{}
Merged Instruction: """.format("".join(middle_inputs))
  # print("combine_prompt: ", combine_prompt)
  tokens = tokenizer(combine_prompt, return_tensors="pt", truncation=True, padding=True).to(0)
  outputs = model.generate(input_ids=tokens["input_ids"], attention_mask= tokens["attention_mask"], max_new_tokens = 50, eos_token_id= tokenizer.eos_token_id, pad_token_id = tokenizer.pad_token_id)
  str_output = tokenizer.decode(outputs[0], skip_special_tokens=True)
  # print(str_output)
  combined_instruction = str_output.split("Example2:")[1].split("Merged Instruction:")[1].strip().split("\n")[0]
  # combined_instruction = str_output.split("Instruction: ")[1].replace("\n", " ").replace("\r", " ").strip()
  return combined_instruction#combined_instruction

"""# Dialogue Generation: Main function"""

# Intent Instructions

intents_dict = {
    "CQ": {
        "user instruction": "Reply with one question asking for clarification in conversation style.",
        "agent instruction": "Reply with one follow-up response in conversation style.",
        "user generation": "Question:",
        "agent generation": "Response:"
    },
    "FD": {
        "user instruction": "Reply with more details in conversation style.",
        "agent instruction": "Reply with further details in conversation style.",
        "user generation": "Response:",
        "agent generation": "Response:"
    },
    "GG": {
        "user instruction": "Continue the conversation by expressing gratitude for the agent's help.",
        "agent instruction": "Continue the conversation by expressing gratitude for the user's questions.",
        "user generation": "Gratitude:",
        "agent generation": "Gratitude:"
    },
    "PA": {
        "user instruction": "Provide a potential solution or answer in conversation style.",
        "agent instruction": "Provide a potential solution or answer in conversation style.",
        "user generation": "Response:",
        "agent generation": "Response:"
    },
    "IR": {
        "user instruction": "Reply with relevant information.",
        "agent instruction": "Ask the user to provide relevant information needed for their previous question.",
        "user generation": "Response:",
        "agent generation": "Question:"
    },
    "OQ": {
        "user instruction": "Formulate the first question posed by a user that initiates a QA dialogue.",
        "agent instruction": "Formulate an original question posed by an agent.",
        "user generation": "Question:",
        "agent generation": "Question:"
    },
    "FQ": {
        "user instruction": "Formulate a follow-up question from a user, seeking further clarification or information.",
        "agent instruction": "Formulate a follow-up question from an agent, seeking further clarification or information.",
        "user generation": "Question:",
        "agent generation": "Question:"
    },
    "RQ": {
        "user instruction": "Now you are talking from the point of view of a third participant in the conversation. Repeat Question: .",
        "agent instruction": "Now you are talking from the point of view of a third participant in the conversation. Repeat Question: .",
        "user generation": "Third Participant:",
        "agent generation": "Third Participant:"
    },
    "PF": {
        "user instruction": "Express satisfaction and appreciation for a working solution.",
        "agent instruction": "Express satisfaction and appreciation for the conversation.",
        "user generation": "Feedback:",
        "agent generation": "Feedback:"
    },
    "NF": {
        "user instruction": "Convey dissatisfaction for the previous response.",
        "agent instruction": "Convey dissatisfaction for the previous response.",
        "user generation": "Negative Feedback:",
        "agent generation": "Negative Feedback:"
    },
    "JK": {
        "user instruction": "Reply with gibberish information. It can contain emojis.",
        "agent instruction": "Reply with gibberish information. It can contain emojis.",
        "user generation": "Gibberish:",
        "agent generation": "Gibberish:"
    },
    "O": {
        "user instruction": "Reply with a system error. Return N/A",
        "agent instruction": "Reply with a system error. Return N/A",
        "user generation": "System Error:",
        "agent generation": "System Error:"
    }
}

# Prompt utils
general_instruction_first_turn = "I will give you an entity, its type, and a background document, along with the user's first question to start a QA dialogue."
general_instruction = "I will give you an entity, its type, and a background document, and a conversation history that ends in a "
conversational_turns = ["user", "agent"]

def generate_dialogue(topic, topic_type, context, list_of_intents, first_question):
  combined_instructions = {}
  utterances_with_intents = []
  list_of_intents_original = list(list_of_intents)
  conv_turns = get_turn(list_of_intents)
  # pass OQ here then it will be extracted from last_line :D
  message = general_instruction_first_turn +  intents_dict[list_of_intents[0]]["user instruction"] + "\n" + \
            topic + "\n" + \
            context + "\n" + \
            intents_dict[list_of_intents[0]]["user generation"] + first_question
  str_output = message
  str_output = filter_new_turn(str_output, message, ":", ":")
  str_output = trim_to_last_punctuation(str_output) #filter sentences without end
  key = ":".join(str_output.strip().split("\n")[-1].split(":")[1:]).strip()
  utterances_with_intents.append({key:"OQ"})
  for i in range(1, len(list_of_intents)):

    turn = conversational_turns[conv_turns[i]]  ##IF _ we keep the turn one more round; but using different intents
    # Get the last turn before complex intents
    j = 1
    while True:
        try:
            previous_reply = intents_dict[list_of_intents[i-j]][turn + " generation"].replace(":", ".")
            break  # If the assignment is successful, break out of the loop
        except KeyError:
            j += 1  # Increment j and try the next item
    instruction_to_message = ""
    if "_" in list_of_intents[i]:
      instruction_to_message = combine_instructionv2(list_of_intents[i], intents_dict, intents_dict_secondkey = turn + " instruction")
      if list_of_intents[i] not in combined_instructions:
         combined_instructions[list_of_intents[i]] = {}
      combined_instructions[list_of_intents[i]][turn + " instruction"] = instruction_to_message
      generation_to_message = intents_dict[list_of_intents[i].split("_")[0]][turn + " generation"]
    else:
      instruction_to_message = intents_dict[list_of_intents[i]][turn + " instruction"]
      generation_to_message = intents_dict[list_of_intents[i]][turn + " generation"]
    last_line = str_output.strip().split("\n")[-1]  # Store the last line
    conversation_history = "\n".join(str_output.strip().split("\n"))#[1:-1])
    message = general_instruction + previous_reply + " " + instruction_to_message + "\n" + \
              conversation_history + "\n" + \
              generation_to_message
    str_output = turn_generation(message)
    str_output = filter_new_turn(str_output, message, ":", ":")
    str_output = trim_to_last_punctuation(str_output)
    key = ":".join(str_output.strip().split("\n")[-1].split(":")[1:]).strip()
    value = list_of_intents_original[i]
    utterances_with_intents.append({key:value})
  utterances_with_intents = {"generated_dialogue": utterances_with_intents, "entity_card_obj": None}
  return utterances_with_intents, combined_instructions

"""# Dialogue Generation: Itteration"""

import jsonlines

import sys

if is_colab:
  path_entity_cards = "./synthetic_entity_cards_with_intents_zephyr7beta.jsonl_00"
else:
  path_entity_cards = sys.argv[1]
path_generated_dialogues = path_entity_cards + "_gen_dialogues.jsonl"

# cnt = 0
all_combined_instructions = {}
with jsonlines.open(path_generated_dialogues, mode='w') as writer:
  with jsonlines.open(path_entity_cards) as reader:
    for obj in tqdm.tqdm(reader, desc="itterating on entity cards and sequence of intents..."):
      # print(obj)
      try:
        cnt += 1
        utterances_with_intents, combined_instructions = generate_dialogue(obj["entity_name"].strip(), obj["entity_type"].strip(), obj["entity_description"].strip(), obj["sequence_of_intents"], obj["conversation_starter"].strip())
        utterances_with_intents["entity_card_obj"] = obj
        all_combined_instructions = all_combined_instructions | combined_instructions
        writer.write(utterances_with_intents)
        # if cnt>3:
          # break
      except Exception as e:
        print("skipp due to the error in parsing object: ", obj)
        pass
        continue

import json
path_all_combined_instructions = path_entity_cards + "_all_combined_instructions.json"
with open(path_all_combined_instructions, "w") as fp:
  json.dump(all_combined_instructions, indent= True, fp = fp)
